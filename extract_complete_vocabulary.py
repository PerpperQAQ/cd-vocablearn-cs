#!/usr/bin/env python3
"""
å®Œæ•´æå–ç½‘ç«™æ‰€æœ‰è¯æ±‡å’Œæ–‡æœ¬å†…å®¹
åŒ…æ‹¬å•è¯æœ¬èº«ã€å®šä¹‰ã€ä¾‹å¥ç­‰æ‰€æœ‰éœ€è¦è¯­éŸ³æ’­æŠ¥çš„å†…å®¹
"""

import json
import re
import os

def clean_filename(text):
    """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦"""
    # æ›¿æ¢ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦ä¸ºä¸‹åˆ’çº¿
    clean = re.sub(r'[^a-zA-Z0-9\-]', '_', text.lower())
    # ç§»é™¤è¿ç»­çš„ä¸‹åˆ’çº¿
    clean = re.sub(r'_+', '_', clean)
    # ç§»é™¤å¼€å¤´å’Œç»“å°¾çš„ä¸‹åˆ’çº¿
    clean = clean.strip('_')
    return clean

def extract_terms_from_js_file(file_path):
    """ä»JSæ–‡ä»¶ä¸­æå–è¯æ±‡æ•°æ®"""
    terms = []
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–è¯æ±‡å¯¹è±¡
        # åŒ¹é… { ... } çš„å®Œæ•´å¯¹è±¡
        object_pattern = r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}'
        matches = re.findall(object_pattern, content, re.DOTALL)
        
        for match in matches:
            try:
                # æå–å…³é”®å­—æ®µ
                term_match = re.search(r'term:\s*["\']([^"\']+)["\']', match)
                definition_match = re.search(r'definition:\s*["\']([^"\']+)["\']', match)
                example_match = re.search(r'example:\s*["\']([^"\']+)["\']', match)
                translation_match = re.search(r'translation:\s*["\']([^"\']+)["\']', match)
                explanation_match = re.search(r'explanation:\s*["\']([^"\']+)["\']', match)
                
                if term_match:
                    term_data = {
                        'term': term_match.group(1),
                        'definition': definition_match.group(1) if definition_match else '',
                        'example': example_match.group(1) if example_match else '',
                        'translation': translation_match.group(1) if translation_match else '',
                        'explanation': explanation_match.group(1) if explanation_match else ''
                    }
                    terms.append(term_data)
                    
            except Exception as e:
                print(f"è§£æè¯æ±‡å¯¹è±¡æ—¶å‡ºé”™: {e}")
                continue
                
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
    
    return terms

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” æ­£åœ¨æå–ç½‘ç«™æ‰€æœ‰è¯æ±‡å’Œæ–‡æœ¬å†…å®¹...")
    
    # æ•°æ®æ–‡ä»¶è·¯å¾„
    data_files = [
        'data/computer-science_terms.js',
        'data/programming_terms.js', 
        'data/artificial-intelligence_terms.js',
        'data/internet_terms.js'
    ]
    
    all_terms = []
    all_text_items = []  # æ‰€æœ‰éœ€è¦è¯­éŸ³æ’­æŠ¥çš„æ–‡æœ¬
    
    # ä»æ¯ä¸ªæ–‡ä»¶æå–è¯æ±‡
    for file_path in data_files:
        if os.path.exists(file_path):
            print(f"ğŸ“– å¤„ç†æ–‡ä»¶: {file_path}")
            terms = extract_terms_from_js_file(file_path)
            all_terms.extend(terms)
            print(f"  æ‰¾åˆ° {len(terms)} ä¸ªè¯æ±‡")
        else:
            print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    print(f"\nğŸ“Š æ€»è®¡æå–è¯æ±‡: {len(all_terms)}")
    
    # æ”¶é›†æ‰€æœ‰éœ€è¦è¯­éŸ³æ’­æŠ¥çš„æ–‡æœ¬
    unique_terms = set()
    term_definitions = []
    term_examples = []
    
    for term_data in all_terms:
        term = term_data['term']
        definition = term_data['definition']
        example = term_data['example']
        
        # æ·»åŠ è¯æ±‡æœ¬èº«
        if term and term not in unique_terms:
            all_text_items.append({
                'type': 'term',
                'text': term,
                'filename': clean_filename(term),
                'priority': 'high'
            })
            unique_terms.add(term)
        
        # æ·»åŠ å®šä¹‰ (å¯é€‰)
        if definition:
            definition_key = f"definition_{clean_filename(term)}"
            term_definitions.append({
                'type': 'definition',
                'text': definition,
                'filename': definition_key,
                'related_term': term,
                'priority': 'medium'
            })
        
        # æ·»åŠ ä¾‹å¥ (å¯é€‰)
        if example:
            example_key = f"example_{clean_filename(term)}"
            term_examples.append({
                'type': 'example', 
                'text': example,
                'filename': example_key,
                'related_term': term,
                'priority': 'low'
            })
    
    # ç”Ÿæˆè¯æ±‡åˆ—è¡¨
    all_vocabulary = list(unique_terms)
    all_vocabulary.sort()
    
    print(f"ğŸ¯ å»é‡åè¯æ±‡æ•°é‡: {len(all_vocabulary)}")
    
    # ä¿å­˜åŸºç¡€è¯æ±‡åˆ—è¡¨
    with open('complete_vocabulary_list.py', 'w', encoding='utf-8') as f:
        f.write('# å®Œæ•´è¯æ±‡åˆ—è¡¨ - æ‰€æœ‰éœ€è¦éŸ³é¢‘çš„å•è¯\n')
        f.write('complete_vocabulary = [\n')
        for term in all_vocabulary:
            f.write(f'    "{term}",\n')
        f.write(']\n\n')
        
        # æ·»åŠ æ–‡ä»¶åæ˜ å°„
        f.write('# è¯æ±‡åˆ°éŸ³é¢‘æ–‡ä»¶åçš„æ˜ å°„\n')
        f.write('vocabulary_mapping = {\n')
        for term in all_vocabulary:
            filename = clean_filename(term)
            f.write(f'    "{term.lower()}": "{filename}.wav",\n')
        f.write('}\n')
    
    # ä¿å­˜å®Œæ•´æ–‡æœ¬å†…å®¹åˆ—è¡¨ (åŒ…æ‹¬å®šä¹‰å’Œä¾‹å¥)
    complete_texts = all_text_items + term_definitions + term_examples
    
    with open('complete_text_content.py', 'w', encoding='utf-8') as f:
        f.write('# å®Œæ•´æ–‡æœ¬å†…å®¹ - åŒ…æ‹¬è¯æ±‡ã€å®šä¹‰ã€ä¾‹å¥\n')
        f.write('complete_text_content = [\n')
        for item in complete_texts:
            f.write(f'    {{\n')
            f.write(f'        "type": "{item["type"]}",\n')
            f.write(f'        "text": """{item["text"]}""",\n')
            f.write(f'        "filename": "{item["filename"]}",\n')
            f.write(f'        "priority": "{item["priority"]}"\n')
            if 'related_term' in item:
                f.write(f'        "related_term": "{item["related_term"]}",\n')
            f.write(f'    }},\n')
        f.write(']\n')
    
    # ç”Ÿæˆè¯¦ç»†çš„JSONæŠ¥å‘Š
    report = {
        'total_terms': len(all_vocabulary),
        'total_texts': len(complete_texts),
        'terms_only': len(all_text_items),
        'definitions': len(term_definitions),
        'examples': len(term_examples),
        'vocabulary_list': all_vocabulary,
        'file_mapping': {term.lower(): clean_filename(term) + '.wav' for term in all_vocabulary}
    }
    
    with open('vocabulary_report.json', 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    print("\n" + "="*60)
    print("ğŸ“Š è¯æ±‡æå–ç»Ÿè®¡:")
    print(f"  ğŸ“ æ€»è¯æ±‡æ•°: {len(all_vocabulary)}")
    print(f"  ğŸ“– å®šä¹‰æ•°: {len(term_definitions)}")  
    print(f"  ğŸ“„ ä¾‹å¥æ•°: {len(term_examples)}")
    print(f"  ğŸ“š æ€»æ–‡æœ¬é¡¹: {len(complete_texts)}")
    
    print(f"\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"  ğŸ“„ complete_vocabulary_list.py - åŸºç¡€è¯æ±‡åˆ—è¡¨")
    print(f"  ğŸ“„ complete_text_content.py - å®Œæ•´æ–‡æœ¬å†…å®¹")
    print(f"  ğŸ“„ vocabulary_report.json - è¯¦ç»†ç»Ÿè®¡æŠ¥å‘Š")
    
    print(f"\nğŸµ æ¨èç”ŸæˆéŸ³é¢‘çš„ä¼˜å…ˆçº§:")
    print(f"  ğŸ”¥ é«˜ä¼˜å…ˆçº§: è¯æ±‡æœ¬èº« ({len(all_text_items)} ä¸ª)")
    print(f"  ğŸ”¹ ä¸­ä¼˜å…ˆçº§: è¯æ±‡å®šä¹‰ ({len(term_definitions)} ä¸ª)")
    print(f"  ğŸ”¸ ä½ä¼˜å…ˆçº§: ä¾‹å¥ ({len(term_examples)} ä¸ª)")

if __name__ == "__main__":
    main() 